import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from ebooklib import epub

def initialize_epub(title, author):
    book = epub.EpubBook()
    book.set_identifier('id123456')
    book.set_language('en')
    book.set_title(title)
    book.add_author(author)
    return book


def fetch_url(session, url, headers):
    response = session.get(url, headers=headers)
    response.raise_for_status()
    return BeautifulSoup(response.text, 'html.parser')


def search_novelhall(session, url, query, headers):
    search_url = f"{url}/index.php?s=so&module=book&keyword={query}"
    search_soup = fetch_url(session, search_url, headers)

    search_results = []
    for row in search_soup.select('.section3 table tbody tr'):
        title = row.select_one('td:nth-child(2) a').text.strip()
        href = row.select_one('td:nth-child(2) a')['href']
        search_results.append((title, href))

    return search_results  # Return base_url as well


def scrape_novelhall(session, url, headers, start_chapter, chapters_to_scrape):
    soup = fetch_url(session, url, headers)
    title = soup.select_one('.book-info h1').text.strip()
    img_url = soup.select_one('.book-img img')['src']
    img_response = session.get(img_url, headers=headers)

    chapter_links = [urljoin(url, elem['href']) for elem in soup.select(
        '#morelist li a')[start_chapter:start_chapter + chapters_to_scrape]]
    chapters = []
    for chapter_url in chapter_links:
        chapter_soup = fetch_url(session, chapter_url, headers)
        chapter_text = "".join(str(tag)
                               for tag in chapter_soup.select('.entry-content'))
        chapters.append(chapter_text)

    return title, img_response.content, chapters

def search_novelbin(session, url, query, headers):
    # Make an AJAX request to the provided search URL
    search_url = f"https://novelbin.me/ajax/search-novel?keyword={query}"
    search_soup = fetch_url(session, search_url, headers)

    # Parse the HTML to extract search results
    search_results = []
    for anchor in search_soup.select('a.list-group-item'):
        title = anchor.get('title').strip()
        href = anchor['href']
        
        # Check the path of the URL to filter out the 'See more results' link
        parsed_url = urlparse(href)
        if parsed_url.path == '/search':
            continue
        
        search_results.append((title, href))
    
    return search_results


def scrape_novelbin(session, url, headers, start_chapter, chapters_to_scrape):
    with requests.Session() as session:
        # Fetch the novel main page
        soup = fetch_url(session, url, headers)
        
        # Extract the title
        title = soup.select_one('.desc h3.title').text.strip()
        
        
        # Extract image URL and fetch the image content
        img_url = soup.select_one('.books .book img')['src']
        img_response = session.get(img_url, headers=headers)
        
        # Get the novelId and construct the chapter list URL
        novel_id = url.split('/')[-1]
        chapter_list_url = f"https://novelbin.me/ajax/chapter-archive?novelId={novel_id}"
        chapter_soup = fetch_url(session, chapter_list_url, headers)
        chapter_links = [a['href'] for a in chapter_soup.select('ul.list-chapter li a')]
        
        # Select the required chapters based on start_chapter and chapters_to_scrape
        chapters_to_fetch = chapter_links[start_chapter:start_chapter + chapters_to_scrape] if chapters_to_scrape else chapter_links[start_chapter:]
        
        chapters = []
        for chapter_url in chapters_to_fetch:
            chapter_soup = fetch_url(session, chapter_url, headers)
            chapter_content_div = chapter_soup.find('div', {'id': 'chr-content'})

            # Remove the <script> and <div> tags related to ads
            for unwanted_tag in chapter_content_div.find_all(['script', 'div']):
                unwanted_tag.decompose()

            chapter_text = "".join(str(tag) for tag in chapter_content_div.find_all('p'))
            chapters.append(chapter_text)

    return title, img_response.content, chapters


def get_intro():
    c1 = epub.EpubHtml(title='Introduction',
                       file_name='intro.xhtml', lang='en')
    c1.content = '''
    <h1>Introduction</h1>
    <p>This book was programmatically generated by Ari. The content you will read in the following chapters has been scraped from various online sources and compiled into an EPUB format for easier reading and accessibility.</p>
    <p>The idea behind this automation is to make a wide array of information readily available in a format that can be conveniently consumed. Each chapter in this book corresponds to different web pages or articles from the source URL provided.</p>
    <p>We hope you find this compilation informative and enjoyable!</p>
    '''
    return c1


def save_epub(book, title):
    if not os.path.exists('books'):
        os.makedirs('books')
    epub.write_epub(os.path.join('books', f'{title}.epub'), book, {})

SOURCES = {
    'NovelHall': {
        'base_url': 'https://www.novelhall.com',
        'search': search_novelhall,
        'scrape': scrape_novelhall
    },
    'NovelBin': {
        'base_url': 'https://novelbin.me',
        'search': search_novelbin,
        'scrape': scrape_novelbin
    }
}

def main():
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        session = requests.Session()

        # Show sources to select
        print("Select a source:")
        for i, source in enumerate(SOURCES.keys()):
            print(f"{i + 1}. {source}")

        source_choice = int(input("Enter the number of the source: "))
        if source_choice < 1 or source_choice > len(SOURCES):
            print("Invalid choice.")
            return

        selected_source = list(SOURCES.keys())[source_choice - 1]
        base_url = SOURCES[selected_source]['base_url']

        query = input("Enter the book title to search: ")
        search_results = SOURCES[selected_source]['search'](session, base_url, query, headers)

        if not search_results:
            print("No results found.")
            return

        print("Search Results:")
        for i, (title, _) in enumerate(search_results):
            print(f"{i+1}. {title}")

        choice = int(input("Select a book to scrape (enter the number): "))
        if choice < 1 or choice > len(search_results):
            print("Invalid choice.")
            return

        _, relative_url = search_results[choice - 1]
        url = urljoin(base_url, relative_url)

        start_chapter = int(input("Enter the starting chapter: ")) - 1
        chapters_to_scrape = int(
            input("Enter the number of chapters to scrape: "))

        title, cover_image, chapters = SOURCES[selected_source]['scrape'](
            session, url, headers, start_chapter, chapters_to_scrape)

        book = initialize_epub(title, "Ari")
        book.set_cover("image.jpg", cover_image)

        intro = get_intro()
        book.add_item(intro)

        chapter_items = [intro]

        for i, chapter_text in enumerate(chapters):
            chapter_title = f'Chapter {start_chapter + i + 1}'
            chapter_item = epub.EpubHtml(
                title=chapter_title, file_name=f'chap_{start_chapter + i + 1}.xhtml', lang='en')
            chapter_item.content = f'<h1>{chapter_title}</h1>{chapter_text}'
            book.add_item(chapter_item)
            chapter_items.append(chapter_item)

        book.toc = chapter_items
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        book.spine = ['nav'] + chapter_items

        save_epub(book, title)

        print(f"Scraped data and generated EPUB for {title}")

    except Exception as e:
        print(f'An error occurred: {e}')


if __name__ == '__main__':
    main()
